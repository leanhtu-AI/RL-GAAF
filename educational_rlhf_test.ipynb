{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Educational RLHF Framework - Test Notebook\n",
    "\n",
    "**Má»¥c Ä‘Ã­ch:** Test tá»«ng component riÃªng biá»‡t trÆ°á»›c khi cháº¡y full system\n",
    "\n",
    "**Thá»© tá»± test:**\n",
    "1. âœ… API Connection & Basic Generation\n",
    "2. âœ… Fixed ArmoRM Component\n",
    "3. âœ… Policy Configuration\n",
    "4. âœ… Single RLHF Iteration\n",
    "5. âœ… Full System Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.178.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.16.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fptshop\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core->google-generativeai) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (2.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lib (C:\\Users\\FPTSHOP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\users\\fptshop\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torchlight-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~lib (C:\\Users\\FPTSHOP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lib (C:\\Users\\FPTSHOP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai python-dotenv numpy pandas matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Imports successful!\n",
      "ðŸ”— Google GenerativeAI version: 0.8.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ðŸ“¦ Imports successful!\")\n",
    "print(f\"ðŸ”— Google GenerativeAI version: {genai.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ API Keys Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flash API key: AIzaSyBINk...\n",
      "âœ… Pro API key: AIzaSyC45g...\n",
      "\n",
      "ðŸŽ¯ API configuration complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set API keys directly (for testing)\n",
    "FLASH_API_KEY = \"AIzaSyBINk0rcDZIvLtYezaFirKmpofiy5MRVs0\"\n",
    "\n",
    "# Validate keys\n",
    "if not FLASH_API_KEY:\n",
    "    print(\"âš ï¸  Please set FLASH_API_KEY properly.\")\n",
    "else:\n",
    "    print(f\"âœ… Flash API key: {FLASH_API_KEY[:10]}...\")\n",
    "\n",
    "if not PRO_API_KEY:\n",
    "    print(\"âš ï¸  Please set PRO_API_KEY properly.\")\n",
    "else:\n",
    "    print(f\"âœ… Pro API key: {PRO_API_KEY[:10]}...\")\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=FLASH_API_KEY)\n",
    "flash_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "pro_model   = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "print(\"\\nðŸŽ¯ API configuration complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test 1: Basic API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gemini Flash...\n",
      "âœ… Flash response: Machine learning is a type of artificial intelligence that allows software applications to become mo...\n",
      "\n",
      "Testing Gemini Pro...\n",
      "âœ… Pro response: Machine learning is a subset of artificial intelligence that enables computer systems to learn from ...\n",
      "\n",
      "ðŸŽ¯ API Test Result: PASSED\n"
     ]
    }
   ],
   "source": [
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "]\n",
    "\n",
    "\n",
    "async def test_basic_generation():\n",
    "    \"\"\"Test if models can generate basic responses\"\"\"\n",
    "    \n",
    "    test_prompt = \"What is machine learning? Give a brief answer.\"\n",
    "    \n",
    "    try:\n",
    "        # Test Flash model\n",
    "        print(\"Testing Gemini Flash...\")\n",
    "        flash_response = await flash_model.generate_content_async(\n",
    "            test_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=2048,\n",
    "                candidate_count=1\n",
    "            ),\n",
    "            safety_settings={\n",
    "                \"HARASSMENT\": \"block_none\",\n",
    "                \"HATE\": \"block_none\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if flash_response.candidates:\n",
    "            candidate = flash_response.candidates[0]\n",
    "            if candidate.content and candidate.content.parts:\n",
    "                flash_text = candidate.content.parts[0].text\n",
    "                print(f\"âœ… Flash response: {flash_text[:100]}...\")\n",
    "            else:\n",
    "                print(f\"âŒ Flash response missing parts or content. Full candidate: {candidate}\")\n",
    "        else:\n",
    "            print(\"âŒ Flash response has no candidates.\")\n",
    "            print(f\"ðŸ” Full Flash response: {flash_response}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Flash error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Test Pro model (for ArmoRM)\n",
    "        print(\"\\nTesting Gemini Pro...\")\n",
    "        pro_response = await pro_model.generate_content_async(\n",
    "            test_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=2048\n",
    "            ),\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "                \n",
    "        if pro_response.candidates:\n",
    "            candidate = pro_response.candidates[0]\n",
    "            if candidate.content and candidate.content.parts:\n",
    "                pro_text = candidate.content.parts[0].text\n",
    "                print(f\"âœ… Pro response: {pro_text[:100]}...\")\n",
    "            else:\n",
    "                print(f\"âŒ Pro response missing parts or content. Full candidate: {candidate}\")\n",
    "        else:\n",
    "            print(\"âŒ Pro response has no candidates.\")\n",
    "            print(f\"ðŸ” Full Pro response: {pro_response}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pro error: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "api_test_result = await test_basic_generation()\n",
    "print(f\"\\nðŸŽ¯ API Test Result: {'PASSED' if api_test_result else 'FAILED'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test 2: Fixed ArmoRM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any\n",
    "import asyncio\n",
    "import json\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerationConfig\n",
    "\n",
    "@dataclass\n",
    "class RewardComponents:\n",
    "    helpfulness: float = 0.0\n",
    "    correctness: float = 0.0\n",
    "    clarity: float = 0.0\n",
    "\n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"helpfulness\": self.helpfulness,\n",
    "            \"correctness\": self.correctness,\n",
    "            \"clarity\": self.clarity\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class ArmoRMResult:\n",
    "    prompt: str\n",
    "    response: str\n",
    "    reward_components: RewardComponents\n",
    "    composite_score: float\n",
    "    reasoning: str = \"\"\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class TestArmoRM:\n",
    "    def __init__(self, pro_model):\n",
    "        self.pro_model = pro_model\n",
    "        self.weights = {\n",
    "            \"helpfulness\": 0.4,\n",
    "            \"correctness\": 0.4,\n",
    "            \"clarity\": 0.2\n",
    "        }\n",
    "        self.safety_settings = [\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\",       \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\",      \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\"threshold\": \"BLOCK_NONE\"},\n",
    "        ]\n",
    "\n",
    "    def _create_eval_prompt(self, prompt: str, response: str) -> str:\n",
    "        return (\n",
    "            f\"Evaluate this response (0-10) for helpfulness, correctness, clarity.\\n\\n\"\n",
    "            f\"QUESTION: {prompt}\\n\\n\"\n",
    "            f\"RESPONSE: {response}\\n\\n\"\n",
    "            \"Return ONLY JSON: {\\\"helpfulness\\\":7.5, \\\"correctness\\\":8.0, \\\"clarity\\\":7.0}\"\n",
    "        )\n",
    "\n",
    "    def _clean_json(self, text: str) -> str:\n",
    "        s, e = text.find('{'), text.rfind('}') + 1\n",
    "        return text[s:e] if s != -1 and e > s else '{\"helpfulness\":5,\"correctness\":5,\"clarity\":5}'\n",
    "\n",
    "    def _validate_scores(self, scores: Dict) -> Dict[str, float]:\n",
    "        out = {}\n",
    "        for k in self.weights:\n",
    "            try:\n",
    "                v = float(scores.get(k, 5.0))\n",
    "                out[k] = max(0.0, min(10.0, v))\n",
    "            except (ValueError, TypeError):\n",
    "                out[k] = 5.0\n",
    "        return out\n",
    "\n",
    "    def _compute_composite(self, comp: RewardComponents) -> float:\n",
    "        d = comp.to_dict()\n",
    "        return round(sum(self.weights[k] * d[k] for k in d), 2)\n",
    "\n",
    "    def _create_fallback_result(self, prompt: str, response: str) -> ArmoRMResult:\n",
    "        length_score = min(8.0, 4.0 + len(response.split()) / 50)\n",
    "        ex = 1.0 if \"example\" in response.lower() else 0.0\n",
    "        code = 1.0 if \"```\" in response else 0.0\n",
    "        base = 5.0 + 0.5 * ex + 0.5 * code\n",
    "        comps = RewardComponents(\n",
    "            helpfulness=min(10.0, base + length_score / 10),\n",
    "            correctness=base,\n",
    "            clarity=min(10.0, base + 0.3)\n",
    "        )\n",
    "        return ArmoRMResult(\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "            reward_components=comps,\n",
    "            composite_score=self._compute_composite(comps),\n",
    "            reasoning=\"Fallback heuristic evaluation\",\n",
    "            metadata={\"method\": \"fallback\"}\n",
    "        )\n",
    "\n",
    "    async def evaluate_response(self, prompt: str, response: str) -> ArmoRMResult:\n",
    "        eval_prompt = self._create_eval_prompt(prompt, response)\n",
    "\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                result = await self.pro_model.generate_content_async(\n",
    "                    eval_prompt,\n",
    "                    generation_config=GenerationConfig(\n",
    "                        temperature=0.1,\n",
    "                        max_output_tokens=2048,\n",
    "                        candidate_count=1\n",
    "                    ),\n",
    "                    safety_settings=self.safety_settings\n",
    "                )\n",
    "\n",
    "                cands = getattr(result, \"candidates\", []) or []\n",
    "                if not cands:\n",
    "                    print(f\"Attempt {attempt+1}: no candidates, retry\")\n",
    "                    await asyncio.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                content = cands[0].content\n",
    "\n",
    "                # âœ… Láº¥y text tá»« .text hoáº·c .parts[0].text\n",
    "                raw = getattr(content, \"text\", None)\n",
    "                if not raw and hasattr(content, \"parts\") and content.parts:\n",
    "                    raw = content.parts[0].text\n",
    "\n",
    "                if not raw:\n",
    "                    print(f\"Attempt {attempt+1}: no usable content, retry\")\n",
    "                    await asyncio.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                raw = raw.strip()\n",
    "                print(f\"[Armo][Attempt {attempt+1}] Raw JSON: {raw[:80]}...\")\n",
    "\n",
    "                cleaned = self._clean_json(raw)\n",
    "                scores  = json.loads(cleaned)\n",
    "                valid   = self._validate_scores(scores)\n",
    "\n",
    "                comps     = RewardComponents(**valid)\n",
    "                composite = self._compute_composite(comps)\n",
    "\n",
    "                return ArmoRMResult(\n",
    "                    prompt=prompt,\n",
    "                    response=response,\n",
    "                    reward_components=comps,\n",
    "                    composite_score=composite,\n",
    "                    reasoning=f\"api ok #{attempt+1}\",\n",
    "                    metadata={\"attempt\": attempt+1, \"method\": \"api\"}\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt+1}: error - {e}\")\n",
    "                await asyncio.sleep(1)\n",
    "\n",
    "        return self._create_fallback_result(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing ArmoRM Evaluation...\n",
      "\n",
      "--- Test Case 1 ---\n",
      "Prompt: What is LangChain?\n",
      "Response: LangChain is a framework for building applications with large language models. I...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\"helpfulness\":7.5, \"correctness\":8.0, \"clarity\":7.0}\n",
      "```...\n",
      "Composite Score: 7.6\n",
      "Components: {'helpfulness': 7.5, 'correctness': 8.0, 'clarity': 7.0}\n",
      "Method: api\n",
      "\n",
      "--- Test Case 2 ---\n",
      "Prompt: How to create a simple chatbot?\n",
      "Response: Use LangChain components to build a chatbot. Import the necessary modules and co...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\n",
      "  \"helpfulness\": 6.0,\n",
      "  \"correctness\": 9.0,\n",
      "  \"clarity\": 8.0\n",
      "}\n",
      "```...\n",
      "Composite Score: 7.6\n",
      "Components: {'helpfulness': 6.0, 'correctness': 9.0, 'clarity': 8.0}\n",
      "Method: api\n",
      "\n",
      "ðŸŽ¯ ArmoRM Test Results:\n",
      "   Average Score: 7.6\n",
      "   Tests Completed: 2\n",
      "\n",
      "ðŸŽ¯ ArmoRM Test: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Test ArmoRM\n",
    "async def test_armo_rm():\n",
    "    \"\"\"Test ArmoRM evaluation\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing ArmoRM Evaluation...\")\n",
    "    \n",
    "    # Initialize ArmoRM\n",
    "    armo = TestArmoRM(pro_model)\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"prompt\": \"What is LangChain?\",\n",
    "            \"response\": \"LangChain is a framework for building applications with large language models. It provides tools for chaining operations, managing prompts, and integrating with various data sources. For example, you can use it to create chatbots or document analysis systems.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"How to create a simple chatbot?\", \n",
    "            \"response\": \"Use LangChain components to build a chatbot. Import the necessary modules and configure your model.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\n--- Test Case {i+1} ---\")\n",
    "        print(f\"Prompt: {test_case['prompt']}\")\n",
    "        print(f\"Response: {test_case['response'][:80]}...\")\n",
    "        \n",
    "        # Evaluate\n",
    "        result = await armo.evaluate_response(\n",
    "            test_case['prompt'],\n",
    "            test_case['response']\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Show results\n",
    "        print(f\"Composite Score: {result.composite_score:.1f}\")\n",
    "        print(f\"Components: {result.reward_components.to_dict()}\")\n",
    "        print(f\"Method: {result.metadata.get('method', 'unknown')}\")\n",
    "        \n",
    "        # Small delay\n",
    "        await asyncio.sleep(2)\n",
    "    \n",
    "    # Summary\n",
    "    avg_score = np.mean([r.composite_score for r in results])\n",
    "    print(f\"\\nðŸŽ¯ ArmoRM Test Results:\")\n",
    "    print(f\"   Average Score: {avg_score:.1f}\")\n",
    "    print(f\"   Tests Completed: {len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run ArmoRM test\n",
    "if api_test_result:\n",
    "    armo_results = await test_armo_rm()\n",
    "    armo_test_passed = len(armo_results) > 0 and all(r.composite_score > 0 for r in armo_results)\n",
    "    print(f\"\\nðŸŽ¯ ArmoRM Test: {'PASSED' if armo_test_passed else 'FAILED'}\")\n",
    "else:\n",
    "    print(\"â­ï¸ Skipping ArmoRM test (API test failed)\")\n",
    "    armo_test_passed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test 3: Policy Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Policy Generation...\n",
      "\n",
      "--- Testing conservative policy ---\n",
      "âœ… Success! Response length: 44 words\n",
      "Preview: Machine learning is a type of artificial intelligence that allows software applications to become mo...\n",
      "\n",
      "--- Testing creative policy ---\n",
      "âœ… Success! Response length: 441 words\n",
      "Preview: Hey there!  Ready to dive into the amazing world of machine learning?  Forget robots taking over the...\n",
      "\n",
      "ðŸŽ¯ Policy Test Results:\n",
      "   Successful Policies: 2/2\n",
      "\n",
      "ðŸŽ¯ Policy Test: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Policy Configuration Test\n",
    "@dataclass\n",
    "class PolicyConfig:\n",
    "    \"\"\"Fixed policy configuration\"\"\"\n",
    "    model_name: str = \"gemini-1.5-flash\"\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    max_output_tokens: int = 2048\n",
    "    policy_type: str = \"main\"\n",
    "    system_prompt: str = \"\"\n",
    "    \n",
    "    def to_generation_config(self):\n",
    "        \"\"\"Convert to GenerationConfig without thinking_budget\"\"\"\n",
    "        return genai.GenerationConfig(\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            max_output_tokens=self.max_output_tokens,\n",
    "            candidate_count=1\n",
    "        )\n",
    "\n",
    "async def test_policy_generation():\n",
    "    \"\"\"Test policy-based content generation\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Policy Generation...\")\n",
    "    \n",
    "    # Create test policies\n",
    "    policies = {\n",
    "        \"conservative\": PolicyConfig(\n",
    "            temperature=0.3,\n",
    "            top_p=0.8,\n",
    "            max_output_tokens=2048,\n",
    "            system_prompt=\"You are a helpful and precise AI tutor. Give accurate, concise answers.\",\n",
    "            policy_type=\"conservative\"\n",
    "        ),\n",
    "        \"creative\": PolicyConfig(\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            max_output_tokens=2048,\n",
    "            system_prompt=\"You are an engaging AI tutor. Use examples and creative explanations.\",\n",
    "            policy_type=\"creative\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    test_prompt = \"Explain what machine learning is\"\n",
    "    results = {}\n",
    "    \n",
    "    for policy_name, policy in policies.items():\n",
    "        print(f\"\\n--- Testing {policy_name} policy ---\")\n",
    "        \n",
    "        try:\n",
    "            # Create full prompt\n",
    "            full_prompt = f\"{policy.system_prompt}\\n\\nUser: {test_prompt}\\n\\nAssistant:\"\n",
    "            \n",
    "            # Generate response\n",
    "            response = await flash_model.generate_content_async(\n",
    "                full_prompt,\n",
    "                generation_config=policy.to_generation_config(),\n",
    "                safety_settings={\n",
    "                    \"HARASSMENT\": \"block_none\",\n",
    "                    \"HATE\": \"block_none\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Check response\n",
    "            if response.candidates and response.candidates[0].content:\n",
    "                text = response.candidates[0].content.parts[0].text.strip()\n",
    "                results[policy_name] = {\n",
    "                    \"success\": True,\n",
    "                    \"response\": text,\n",
    "                    \"length\": len(text.split()),\n",
    "                    \"temperature\": policy.temperature\n",
    "                }\n",
    "                print(f\"âœ… Success! Response length: {len(text.split())} words\")\n",
    "                print(f\"Preview: {text[:100]}...\")\n",
    "            else:\n",
    "                results[policy_name] = {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"Empty response\",\n",
    "                    \"finish_reason\": response.candidates[0].finish_reason if response.candidates else \"No candidates\"\n",
    "                }\n",
    "                print(f\"âŒ Empty response. Finish reason: {results[policy_name]['finish_reason']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            results[policy_name] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "        \n",
    "        # Delay between policies\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    # Summary\n",
    "    successful_policies = sum(1 for r in results.values() if r.get(\"success\"))\n",
    "    print(f\"\\nðŸŽ¯ Policy Test Results:\")\n",
    "    print(f\"   Successful Policies: {successful_policies}/{len(policies)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run policy test\n",
    "if api_test_result:\n",
    "    policy_results = await test_policy_generation()\n",
    "    policy_test_passed = sum(1 for r in policy_results.values() if r.get(\"success\")) > 0\n",
    "    print(f\"\\nðŸŽ¯ Policy Test: {'PASSED' if policy_test_passed else 'FAILED'}\")\n",
    "else:\n",
    "    print(\"â­ï¸ Skipping policy test (API test failed)\")\n",
    "    policy_test_passed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test 4: Single RLHF Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Single RLHF Iteration...\n",
      "\n",
      "--- Collecting tuple 1: What is LangChain? ---\n",
      "Sampling from policy 1...\n",
      "Sampling from policy 2...\n",
      "Response 1: LangChain is a framework for developing applications powered...\n",
      "Response 2: User: What is LangChain?\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Hey there! LangChain i...\n",
      "Evaluating responses...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\"helpfulness\":7.5, \"correctness\":8.0, \"clarity\":7.0}\n",
      "```...\n",
      "Attempt 1: no usable content, retry\n",
      "[Armo][Attempt 2] Raw JSON: ```json\n",
      "{\"helpfulness\":7.5, \"correctness\":8.0, \"clarity\":7.0}\n",
      "```...\n",
      "Preference: Response 2 (strength: 0.00)\n",
      "Scores: R1=7.6, R2=7.6\n",
      "\n",
      "--- Collecting tuple 2: How to create a chatbot with LangChain? ---\n",
      "Sampling from policy 1...\n",
      "Sampling from policy 2...\n",
      "Response 1: Creating a chatbot with LangChain involves several steps, an...\n",
      "Response 2: Let's build a chatbot with LangChain!  We'll explore a few a...\n",
      "Evaluating responses...\n",
      "Attempt 1: no usable content, retry\n",
      "[Armo][Attempt 2] Raw JSON: ```json\n",
      "{\"helpfulness\":7.5, \"correctness\":8.0, \"clarity\":7.0}\n",
      "```...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\"helpfulness\":7.5, \"correctness\":8.0,...\n",
      "Preference: Response 1 (strength: 2.60)\n",
      "Scores: R1=7.6, R2=5.0\n",
      "\n",
      "--- Collecting tuple 3: Explain LangChain chains ---\n",
      "Sampling from policy 1...\n",
      "Sampling from policy 2...\n",
      "Response 1: LangChain chains are sequences of calls to other LangChain o...\n",
      "Response 2: Hey there!  LangChain chains are like assembly lines for you...\n",
      "Evaluating responses...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\"helpfulness\":7.5, \"correctness\":8.0, \"clarity\":7.0}\n",
      "```...\n",
      "Attempt 1: no usable content, retry\n",
      "Attempt 2: no usable content, retry\n",
      "Attempt 3: no usable content, retry\n",
      "Preference: Response 1 (strength: 1.22)\n",
      "Scores: R1=7.6, R2=6.4\n",
      "\n",
      "ðŸŽ¯ Single RLHF Iteration Results:\n",
      "   Tuples Collected: 3\n",
      "   Average Preference Strength: 1.27\n",
      "   Policy 1 Wins: 2\n",
      "   Policy 2 Wins: 1\n",
      "   Responses with Errors: 0\n",
      "\n",
      "ðŸŽ¯ RLHF Iteration Test: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Single RLHF Iteration Test\n",
    "async def test_single_rlhf_iteration():\n",
    "    \"\"\"Test a single RLHF data collection iteration\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Single RLHF Iteration...\")\n",
    "    \n",
    "    if not (api_test_result and armo_test_passed and policy_test_passed):\n",
    "        print(\"â­ï¸ Skipping RLHF test (prerequisites failed)\")\n",
    "        return False\n",
    "    \n",
    "    # Initialize components\n",
    "    armo = TestArmoRM(pro_model)\n",
    "    \n",
    "    # Create two policies for comparison\n",
    "    policy1 = PolicyConfig(\n",
    "        temperature=0.5,\n",
    "        system_prompt=\"You are a precise AI tutor for LangChain.\",\n",
    "        policy_type=\"main\"\n",
    "    )\n",
    "    \n",
    "    policy2 = PolicyConfig(\n",
    "        temperature=0.8,\n",
    "        system_prompt=\"You are a creative AI tutor for LangChain. Use examples.\",\n",
    "        policy_type=\"enhancer\"\n",
    "    )\n",
    "    \n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"What is LangChain?\",\n",
    "        \"How to create a chatbot with LangChain?\",\n",
    "        \"Explain LangChain chains\"\n",
    "    ]\n",
    "    \n",
    "    iteration_data = []\n",
    "    \n",
    "    async def sample_from_policy(policy, prompt):\n",
    "        \"\"\"Sample response from policy with error handling\"\"\"\n",
    "        full_prompt = f\"{policy.system_prompt}\\n\\nUser: {prompt}\\n\\nAssistant:\"\n",
    "        \n",
    "        try:\n",
    "            response = await flash_model.generate_content_async(\n",
    "                full_prompt,\n",
    "                generation_config=policy.to_generation_config(),\n",
    "                safety_settings={\"HARASSMENT\": \"block_none\", \"HATE\": \"block_none\"}\n",
    "            )\n",
    "            \n",
    "            if response.candidates and response.candidates[0].content:\n",
    "                return response.candidates[0].content.parts[0].text.strip()\n",
    "            else:\n",
    "                return f\"EMPTY_RESPONSE (finish_reason: {response.candidates[0].finish_reason if response.candidates else 'no_candidates'})\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    # Collect data for 3 tuples\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\n--- Collecting tuple {i+1}: {prompt} ---\")\n",
    "        \n",
    "        # Generate responses from both policies\n",
    "        print(\"Sampling from policy 1...\")\n",
    "        response1 = await sample_from_policy(policy1, prompt)\n",
    "        \n",
    "        await asyncio.sleep(1)  # Rate limiting\n",
    "        \n",
    "        print(\"Sampling from policy 2...\")\n",
    "        response2 = await sample_from_policy(policy2, prompt)\n",
    "        \n",
    "        await asyncio.sleep(1)  # Rate limiting\n",
    "        \n",
    "        # Show responses\n",
    "        print(f\"Response 1: {response1[:60]}...\")\n",
    "        print(f\"Response 2: {response2[:60]}...\")\n",
    "        \n",
    "        # Evaluate responses with ArmoRM\n",
    "        print(\"Evaluating responses...\")\n",
    "        \n",
    "        eval1 = await armo.evaluate_response(prompt, response1)\n",
    "        await asyncio.sleep(2)  # Rate limiting\n",
    "        \n",
    "        eval2 = await armo.evaluate_response(prompt, response2)\n",
    "        await asyncio.sleep(2)  # Rate limiting\n",
    "        \n",
    "        # Determine preference\n",
    "        if eval1.composite_score > eval2.composite_score:\n",
    "            preference = 1\n",
    "            chosen = response1\n",
    "            rejected = response2\n",
    "            strength = eval1.composite_score - eval2.composite_score\n",
    "        else:\n",
    "            preference = 2\n",
    "            chosen = response2\n",
    "            rejected = response1\n",
    "            strength = eval2.composite_score - eval1.composite_score\n",
    "        \n",
    "        # Store data\n",
    "        tuple_data = {\n",
    "            'prompt': prompt,\n",
    "            'response_1': response1,\n",
    "            'response_2': response2,\n",
    "            'preference': preference,\n",
    "            'preference_strength': strength,\n",
    "            'scores': {\n",
    "                'response_1': eval1.composite_score,\n",
    "                'response_2': eval2.composite_score\n",
    "            },\n",
    "            'evaluations': {\n",
    "                'response_1': eval1,\n",
    "                'response_2': eval2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        iteration_data.append(tuple_data)\n",
    "        \n",
    "        print(f\"Preference: Response {preference} (strength: {strength:.2f})\")\n",
    "        print(f\"Scores: R1={eval1.composite_score:.1f}, R2={eval2.composite_score:.1f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nðŸŽ¯ Single RLHF Iteration Results:\")\n",
    "    print(f\"   Tuples Collected: {len(iteration_data)}\")\n",
    "    \n",
    "    # Analyze preferences\n",
    "    avg_strength = np.mean([d['preference_strength'] for d in iteration_data])\n",
    "    policy1_wins = sum(1 for d in iteration_data if d['preference'] == 1)\n",
    "    policy2_wins = sum(1 for d in iteration_data if d['preference'] == 2)\n",
    "    \n",
    "    print(f\"   Average Preference Strength: {avg_strength:.2f}\")\n",
    "    print(f\"   Policy 1 Wins: {policy1_wins}\")\n",
    "    print(f\"   Policy 2 Wins: {policy2_wins}\")\n",
    "    \n",
    "    # Check for errors\n",
    "    errors = sum(1 for d in iteration_data \n",
    "                if 'ERROR' in d['response_1'] or 'ERROR' in d['response_2'] or 'EMPTY' in d['response_1'] or 'EMPTY' in d['response_2'])\n",
    "    \n",
    "    print(f\"   Responses with Errors: {errors}\")\n",
    "    \n",
    "    success = len(iteration_data) == 3 and errors == 0\n",
    "    return success, iteration_data\n",
    "\n",
    "# Run single iteration test\n",
    "rlhf_success, rlhf_data = await test_single_rlhf_iteration()\n",
    "print(f\"\\nðŸŽ¯ RLHF Iteration Test: {'PASSED' if rlhf_success else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test 5: Full System Integration (Mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Full System Integration...\n",
      "ðŸš€ Starting Mini RLHF Training\n",
      "   Iterations: 2\n",
      "   Samples per iteration: 2\n",
      "   Prompts: 4\n",
      "\n",
      "=== Iteration 1/2 ===\n",
      "Collecting sample 1/2: What is LangChain and why is i...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\"helpfulness\":9.0, \"correctness\":9.5, \"clarity\":9.0}\n",
      "```...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\"helpfulness\":8.5, \"correctness\":9.0, \"clarity\":8.5}\n",
      "```...\n",
      "  Preference: 1 (strength: 0.50)\n",
      "Collecting sample 2/2: How do you create a simple cha...\n",
      "Attempt 1: no usable content, retry\n",
      "[Armo][Attempt 2] Raw JSON: ```json...\n",
      "Attempt 1: no usable content, retry\n",
      "Attempt 2: no usable content, retry\n",
      "Attempt 3: no usable content, retry\n",
      "  Preference: 2 (strength: 1.38)\n",
      "Iteration 1 complete:\n",
      "  Average Score: 7.32\n",
      "  Average Strength: 0.94\n",
      "  Total Historical Data: 2\n",
      "\n",
      "=== Iteration 2/2 ===\n",
      "Collecting sample 1/2: What is LangChain and why is i...\n",
      "Attempt 1: no usable content, retry\n",
      "[Armo][Attempt 2] Raw JSON: ```json\n",
      "{\n",
      "  \"helpfulness\": 9.5,\n",
      "  \"correctness\": 10.0,\n",
      "  \"clarity\": 9.5\n",
      "}\n",
      "```...\n",
      "Attempt 1: no usable content, retry\n",
      "Attempt 2: no usable content, retry\n",
      "Attempt 3: no usable content, retry\n",
      "  Preference: 1 (strength: 3.82)\n",
      "Collecting sample 2/2: How do you create a simple cha...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\n",
      "  \"helpfulness\": 7.5,\n",
      "  \"correctness\": 8.0,\n",
      "  \"clarity\": 7.0\n",
      "}\n",
      "```...\n",
      "[Armo][Attempt 1] Raw JSON: ```json\n",
      "{\"helpfulness\":7.5, \"correctness\":8.0, \"clarity\":7.0}\n",
      "```...\n",
      "  Preference: 2 (strength: 0.00)\n",
      "Iteration 2 complete:\n",
      "  Average Score: 7.69\n",
      "  Average Strength: 1.91\n",
      "  Total Historical Data: 4\n",
      "\n",
      "ðŸŽ¯ Full System Test Results:\n",
      "   Iterations Completed: 2\n",
      "   Total Samples Collected: 4\n",
      "   Final Average Score: 7.69\n",
      "\n",
      "ðŸŽ¯ Full System Test: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Mini Full System Test\n",
    "class MiniRLHFSystem:\n",
    "    \"\"\"Minimal RLHF system for testing\"\"\"\n",
    "    \n",
    "    def __init__(self, flash_model, pro_model):\n",
    "        self.flash_model = flash_model\n",
    "        self.pro_model = pro_model\n",
    "        self.armo = TestArmoRM(pro_model)\n",
    "        self.historical_data = []\n",
    "    \n",
    "    def create_reference_policy(self):\n",
    "        \"\"\"Create reference policy\"\"\"\n",
    "        return PolicyConfig(\n",
    "            temperature=0.7,\n",
    "            system_prompt=\"You are a helpful AI tutor for LangChain.\",\n",
    "            policy_type=\"reference\"\n",
    "        )\n",
    "    \n",
    "    def create_policy_pair(self, iteration: int):\n",
    "        \"\"\"Create main and enhancer policies\"\"\"\n",
    "        \n",
    "        # Main policy (exploitation)\n",
    "        main_policy = PolicyConfig(\n",
    "            temperature=max(0.5, 0.7 - 0.1 * iteration),\n",
    "            system_prompt=\"You are an expert AI tutor for LangChain. Provide clear, accurate explanations.\",\n",
    "            policy_type=\"main\"\n",
    "        )\n",
    "        \n",
    "        # Enhancer policy (exploration)\n",
    "        enhancer_policy = PolicyConfig(\n",
    "            temperature=min(1.0, main_policy.temperature + 0.2),\n",
    "            system_prompt=\"You are a creative AI tutor for LangChain. Use examples and engaging explanations.\",\n",
    "            policy_type=\"enhancer\"\n",
    "        )\n",
    "        \n",
    "        return main_policy, enhancer_policy\n",
    "    \n",
    "    async def sample_from_policy(self, policy, prompt):\n",
    "        \"\"\"Sample response from policy\"\"\"\n",
    "        full_prompt = f\"{policy.system_prompt}\\n\\nUser: {prompt}\\n\\nAssistant:\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.flash_model.generate_content_async(\n",
    "                full_prompt,\n",
    "                generation_config=policy.to_generation_config(),\n",
    "                safety_settings={\"HARASSMENT\": \"block_none\", \"HATE\": \"block_none\"}\n",
    "            )\n",
    "            \n",
    "            if response.candidates and response.candidates[0].content:\n",
    "                return response.candidates[0].content.parts[0].text.strip()\n",
    "            else:\n",
    "                return \"EMPTY_RESPONSE\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    async def collect_preferences(self, policy_pair, prompts, num_samples):\n",
    "        \"\"\"Collect preference data\"\"\"\n",
    "        \n",
    "        main_policy, enhancer_policy = policy_pair\n",
    "        data = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Sample prompt\n",
    "            prompt = prompts[i % len(prompts)]\n",
    "            \n",
    "            print(f\"Collecting sample {i+1}/{num_samples}: {prompt[:30]}...\")\n",
    "            \n",
    "            # Generate responses\n",
    "            response1 = await self.sample_from_policy(main_policy, prompt)\n",
    "            await asyncio.sleep(1)\n",
    "            \n",
    "            response2 = await self.sample_from_policy(enhancer_policy, prompt)\n",
    "            await asyncio.sleep(1)\n",
    "            \n",
    "            # Evaluate\n",
    "            eval1 = await self.armo.evaluate_response(prompt, response1)\n",
    "            await asyncio.sleep(2)\n",
    "            \n",
    "            eval2 = await self.armo.evaluate_response(prompt, response2)\n",
    "            await asyncio.sleep(2)\n",
    "            \n",
    "            # Determine preference\n",
    "            if eval1.composite_score > eval2.composite_score:\n",
    "                preference = 1\n",
    "                strength = eval1.composite_score - eval2.composite_score\n",
    "            else:\n",
    "                preference = 2\n",
    "                strength = eval2.composite_score - eval1.composite_score\n",
    "            \n",
    "            data.append({\n",
    "                'prompt': prompt,\n",
    "                'response_1': response1,\n",
    "                'response_2': response2,\n",
    "                'preference': preference,\n",
    "                'strength': strength,\n",
    "                'scores': [eval1.composite_score, eval2.composite_score]\n",
    "            })\n",
    "            \n",
    "            print(f\"  Preference: {preference} (strength: {strength:.2f})\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    async def run_mini_rlhf(self, prompts, iterations=2, samples_per_iteration=3):\n",
    "        \"\"\"Run mini RLHF training\"\"\"\n",
    "        \n",
    "        print(f\"ðŸš€ Starting Mini RLHF Training\")\n",
    "        print(f\"   Iterations: {iterations}\")\n",
    "        print(f\"   Samples per iteration: {samples_per_iteration}\")\n",
    "        print(f\"   Prompts: {len(prompts)}\")\n",
    "        \n",
    "        all_iterations = []\n",
    "        \n",
    "        for t in range(iterations):\n",
    "            print(f\"\\n=== Iteration {t+1}/{iterations} ===\")\n",
    "            \n",
    "            # Create policy pair\n",
    "            policy_pair = self.create_policy_pair(t)\n",
    "            \n",
    "            # Collect data\n",
    "            iteration_data = await self.collect_preferences(\n",
    "                policy_pair, prompts, samples_per_iteration\n",
    "            )\n",
    "            \n",
    "            # Update historical data\n",
    "            self.historical_data.extend(iteration_data)\n",
    "            \n",
    "            # Analyze iteration\n",
    "            avg_score = np.mean([np.mean(d['scores']) for d in iteration_data])\n",
    "            avg_strength = np.mean([d['strength'] for d in iteration_data])\n",
    "            \n",
    "            iteration_summary = {\n",
    "                'iteration': t,\n",
    "                'data': iteration_data,\n",
    "                'avg_score': avg_score,\n",
    "                'avg_strength': avg_strength,\n",
    "                'total_samples': len(iteration_data)\n",
    "            }\n",
    "            \n",
    "            all_iterations.append(iteration_summary)\n",
    "            \n",
    "            print(f\"Iteration {t+1} complete:\")\n",
    "            print(f\"  Average Score: {avg_score:.2f}\")\n",
    "            print(f\"  Average Strength: {avg_strength:.2f}\")\n",
    "            print(f\"  Total Historical Data: {len(self.historical_data)}\")\n",
    "        \n",
    "        return all_iterations\n",
    "\n",
    "async def test_full_system():\n",
    "    \"\"\"Test full system integration\"\"\"\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Full System Integration...\")\n",
    "    \n",
    "    if not rlhf_success:\n",
    "        print(\"â­ï¸ Skipping full system test (RLHF iteration failed)\")\n",
    "        return False\n",
    "    \n",
    "    # Initialize mini system\n",
    "    mini_rlhf = MiniRLHFSystem(flash_model, pro_model)\n",
    "    \n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"What is LangChain and why is it useful?\",\n",
    "        \"How do you create a simple chatbot with LangChain?\",\n",
    "        \"Explain LangChain chains with an example\",\n",
    "        \"What are LangChain agents?\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Run mini RLHF\n",
    "        results = await mini_rlhf.run_mini_rlhf(\n",
    "            prompts=test_prompts,\n",
    "            iterations=2,\n",
    "            samples_per_iteration=2  # Very small for testing\n",
    "        )\n",
    "        \n",
    "        # Analyze results\n",
    "        total_samples = sum(len(r['data']) for r in results)\n",
    "        final_avg_score = results[-1]['avg_score'] if results else 0\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Full System Test Results:\")\n",
    "        print(f\"   Iterations Completed: {len(results)}\")\n",
    "        print(f\"   Total Samples Collected: {total_samples}\")\n",
    "        print(f\"   Final Average Score: {final_avg_score:.2f}\")\n",
    "        \n",
    "        # Check for success\n",
    "        success = (\n",
    "            len(results) == 2 and  # Completed both iterations\n",
    "            total_samples >= 4 and  # Collected expected samples\n",
    "            final_avg_score > 0  # Got valid scores\n",
    "        )\n",
    "        \n",
    "        return success, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Full system test error: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Run full system test\n",
    "full_system_success, full_results = await test_full_system()\n",
    "print(f\"\\nðŸŽ¯ Full System Test: {'PASSED' if full_system_success else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Overall Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Summary\n",
    "test_results = {\n",
    "    \"API Connection\": api_test_result,\n",
    "    \"ArmoRM Evaluation\": armo_test_passed,\n",
    "    \"Policy Generation\": policy_test_passed,\n",
    "    \"RLHF Iteration\": rlhf_success,\n",
    "    \"Full System\": full_system_success\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š EDUCATIONAL RLHF FRAMEWORK - TEST SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for test_name, result in test_results.items():\n",
    "    status = \"âœ… PASSED\" if result else \"âŒ FAILED\"\n",
    "    print(f\"{test_name:.<30} {status}\")\n",
    "\n",
    "passed_tests = sum(test_results.values())\n",
    "total_tests = len(test_results)\n",
    "\n",
    "print(f\"\\nOverall: {passed_tests}/{total_tests} tests passed ({passed_tests/total_tests*100:.1f}%)\")\n",
    "\n",
    "if passed_tests == total_tests:\n",
    "    print(\"\\nðŸŽ‰ ALL TESTS PASSED! System is ready for production.\")\n",
    "elif passed_tests >= 3:\n",
    "    print(\"\\nâš ï¸  PARTIAL SUCCESS. Core components working, some issues remain.\")\n",
    "else:\n",
    "    print(\"\\nâŒ MAJOR ISSUES. Please check API keys and network connectivity.\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nðŸ“ RECOMMENDATIONS:\")\n",
    "\n",
    "if not api_test_result:\n",
    "    print(\"â€¢ Check API keys and network connectivity\")\n",
    "    print(\"â€¢ Verify API quotas and billing\")\n",
    "\n",
    "if not armo_test_passed:\n",
    "    print(\"â€¢ ArmoRM may need simpler prompts\")\n",
    "    print(\"â€¢ Consider increasing retry delays\")\n",
    "\n",
    "if not policy_test_passed:\n",
    "    print(\"â€¢ Check safety settings\")\n",
    "    print(\"â€¢ Review system prompts for policy violations\")\n",
    "\n",
    "if not rlhf_success:\n",
    "    print(\"â€¢ Reduce batch sizes further\")\n",
    "    print(\"â€¢ Increase delays between API calls\")\n",
    "\n",
    "if not full_system_success:\n",
    "    print(\"â€¢ Consider running with even smaller parameters\")\n",
    "    print(\"â€¢ Check rate limiting and quotas\")\n",
    "\n",
    "if passed_tests == total_tests:\n",
    "    print(\"\\nðŸš€ NEXT STEPS:\")\n",
    "    print(\"â€¢ Scale up batch sizes gradually\")\n",
    "    print(\"â€¢ Add more sophisticated prompts\")\n",
    "    print(\"â€¢ Implement production monitoring\")\n",
    "    print(\"â€¢ Deploy full Educational RLHF system\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Diagnostic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic Information\n",
    "print(\"ðŸ” DIAGNOSTIC INFORMATION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(f\"Environment:\")\n",
    "print(f\"  Python version: {sys.version.split()[0]}\")\n",
    "print(f\"  Google GenerativeAI: {genai.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "\n",
    "print(f\"\\nAPI Configuration:\")\n",
    "print(f\"  Flash API Key: {'Set' if FLASH_API_KEY != 'your_flash_api_key_here' else 'Not Set'}\")\n",
    "print(f\"  Pro API Key: {'Set' if PRO_API_KEY != 'your_pro_api_key_here' else 'Not Set'}\")\n",
    "\n",
    "if 'armo_results' in globals() and armo_results:\n",
    "    print(f\"\\nArmoRM Results:\")\n",
    "    for i, result in enumerate(armo_results):\n",
    "        print(f\"  Test {i+1}: Score {result.composite_score:.1f} (Method: {result.metadata.get('method', 'unknown')})\")\n",
    "\n",
    "if 'policy_results' in globals() and policy_results:\n",
    "    print(f\"\\nPolicy Results:\")\n",
    "    for name, result in policy_results.items():\n",
    "        if result.get('success'):\n",
    "            print(f\"  {name}: âœ… {result.get('length', 0)} words\")\n",
    "        else:\n",
    "            print(f\"  {name}: âŒ {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "if 'rlhf_data' in globals() and rlhf_data:\n",
    "    print(f\"\\nRLHF Iteration Data:\")\n",
    "    for i, data in enumerate(rlhf_data):\n",
    "        print(f\"  Tuple {i+1}: Preference {data['preference']} (Strength: {data['preference_strength']:.2f})\")\n",
    "\n",
    "if 'full_results' in globals() and full_results:\n",
    "    print(f\"\\nFull System Results:\")\n",
    "    for i, iteration in enumerate(full_results):\n",
    "        print(f\"  Iteration {i+1}: {iteration['total_samples']} samples, Avg Score: {iteration['avg_score']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Ready for Production?\n",
    "\n",
    "**If all tests passed**, your Educational RLHF Framework is ready for production deployment!\n",
    "\n",
    "**Next steps:**\n",
    "1. Save this notebook as a reference\n",
    "2. Use the fixed components in your main application\n",
    "3. Gradually scale up batch sizes and iterations\n",
    "4. Implement production monitoring and logging\n",
    "5. Deploy with proper error handling and retry mechanisms\n",
    "\n",
    "**If some tests failed**, review the diagnostic information above and:\n",
    "1. Check API keys and quotas\n",
    "2. Verify network connectivity\n",
    "3. Adjust safety settings if needed\n",
    "4. Reduce batch sizes and increase delays\n",
    "5. Simplify prompts if necessary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
